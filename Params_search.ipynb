{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d314a870-2b96-4c71-9295-d1e670ee6730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb9759b9-8540-4714-a8e6-3b425413bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'hyperparameter/FCS')\n",
    "import fcs_functions as fcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1368a996-1f1c-4dc2-9b0e-f4bb5c3f9c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "764f6c5f-4e7c-41db-b6ae-98990f3823d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## font and color for ploting\n",
    "font = {'family': 'serif', 'color':  'darkred', 'weight': 'normal', 'size': 14}\n",
    "color = [(174, 199, 232), (214, 39, 40)]  \n",
    "\n",
    "for i in range(len(color)):    \n",
    "    r, g, b = color[i]    \n",
    "    color[i] = (r / 255., g / 255., b / 255.)      \n",
    "         \n",
    "\n",
    "## Path of the gauges data (raingauges, surface and wastewater flow guages)\n",
    "##path to the csv data file\n",
    "inputfile1 = r'hyperparameter/FCS_input_files/9modeledStns.csv'\n",
    "\n",
    "\n",
    "##path to the csv file with station information\n",
    "path_sw = r'hyperparameter/FCS_input_files/wastewatercollectorslocation_new.csv'\n",
    "\n",
    "## dataframe from the CVS data file\n",
    "df2 = pd.read_csv(inputfile1, sep=',', decimal='.', index_col=[0], parse_dates=True)\n",
    "df2 = df2.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa91fe78-39e1-4d31-83de-82cf175f699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data about the guaging stations (ID, name, and x, y coordinates)\n",
    "path_rain = r'hyperparameter/FCS_input_files/Rainfall_guageing_stations.csv'\n",
    "Rainguages = fcs.station_data(path_rain)\n",
    "SWguages = fcs.station_data(path_sw)\n",
    "\n",
    "# Lis of stations to be modelled\n",
    "#flow =  ['C01', 'C02', 'C11', 'U05', 'U06', 'U09', 'U11', 'U17', 'U19'] \n",
    "\n",
    "flow =  ['C11' ]\n",
    "\n",
    "lag_times = {'C01': 12,\n",
    "              'C02': 15,\n",
    "              'C11': 15,\n",
    "              'U05': 18,\n",
    "              'U06': 9,\n",
    "              'U09': 21,\n",
    "              'U11': 18,\n",
    "              'U17': 6,\n",
    "              'U19': 9}\n",
    "\n",
    "##Create output folder\n",
    "\n",
    "outfolder=r'hyperparameter/Modelling_result'\n",
    "if not os.path.exists(outfolder):\n",
    "    os.makedirs(outfolder)\n",
    "\n",
    "## Create list of Rainguages IDs\n",
    "rain = []\n",
    "for rg in Rainguages:\n",
    "    rain.append(rg.id)\n",
    "    \n",
    "## Creat list of Surfacwwater flow uages IDs\n",
    "flow_st = []\n",
    "for st in SWguages:\n",
    "    flow_st.append(st.id)\n",
    "\n",
    "\n",
    "names = []\n",
    "rsquared = []\n",
    "WL = ['U06','C02','U09', 'C081','C082', 'C981','C982'] ## list of station of Water level\n",
    "n = 5 ## number of RF stations closest to the flow station considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f98444-af2b-4536-bf2d-fde4bad019b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station C11\n",
      "there is nan in the rainfall data\n",
      "there is nan in df[f]\n",
      "there is nan in df4\n",
      "Information of the dataframe for the modelling:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 305800 entries, 2016-01-01 02:45:00 to 2018-12-31 02:40:00\n",
      "Data columns (total 16 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   C11          305800 non-null  float64\n",
      " 1   C11_lag_24   305800 non-null  float64\n",
      " 2   C11_lag_25   305800 non-null  float64\n",
      " 3   C11_lag_26   305800 non-null  float64\n",
      " 4   C11_lag_27   305800 non-null  float64\n",
      " 5   C11_lag_28   305800 non-null  float64\n",
      " 6   C11_lag_29   305800 non-null  float64\n",
      " 7   C11_lag_30   305800 non-null  float64\n",
      " 8   C11_lag_31   305800 non-null  float64\n",
      " 9   C11_lag_32   305800 non-null  float64\n",
      " 10  C11_lag_33   305800 non-null  float64\n",
      " 11  MAL1-1_rs15  305800 non-null  float64\n",
      " 12  MAL1-3_rs15  305800 non-null  float64\n",
      " 13  MAL1-2_rs15  305800 non-null  float64\n",
      " 14  MAL1-4_rs15  305800 non-null  float64\n",
      " 15  MAL1-5_rs15  305800 non-null  float64\n",
      "dtypes: float64(16)\n",
      "memory usage: 39.7 MB\n",
      "None\n",
      "Observations: 305800\n",
      "Training Observations: 244640\n",
      "Testing Observations: 61160\n",
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]}\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 2.8min\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time= 7.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:703: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END bootstrap=True, max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=800; total time= 5.1min\n",
      "[CV] END bootstrap=False, max_depth=100, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=500; total time=10.5min\n",
      "[CV] END bootstrap=False, max_depth=90, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=400; total time= 9.0min\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=5, n_estimators=800; total time= 6.9min\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=400; total time= 7.8min\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 2.5min\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time= 7.7min\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=900; total time=34.3min\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 5.3min\n",
      "[CV] END bootstrap=True, max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time= 2.8min\n",
      "[CV] END bootstrap=False, max_depth=30, max_features=sqrt, min_samples_leaf=4, min_samples_split=5, n_estimators=400; total time= 7.7min\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=900; total time=34.3min\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 5.5min\n",
      "[CV] END bootstrap=False, max_depth=10, max_features=auto, min_samples_leaf=4, min_samples_split=5, n_estimators=900; total time=34.8min\n",
      "[CV] END bootstrap=True, max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=500; total time= 5.0min\n"
     ]
    }
   ],
   "source": [
    "## Create dataframe of the rainfall of the n number of closest stations\n",
    "df = pd.DataFrame()\n",
    "for f in flow:\n",
    "    print('station', f)\n",
    "    \n",
    "    if (f == st.id for st in SWguages):\n",
    "        df_list = fcs.closest_stations(Rainguages, SWguages, f,n)\n",
    "        #print('the flow station and the 10 closest RF stations are ' , df_list)\n",
    "        df_list = [x for x in df_list if x  in df2.columns.values]   #df2 is the dataframe containing the data read from the CSV\n",
    "        df = df2[df_list].copy() # the final dataframe for the DDM\n",
    "        names.append(f)\n",
    "    else:\n",
    "          print('No {0} in flow stations list'.format(f))  \n",
    "    \n",
    "    # ## if there are missing values, fill them with the mean of the series\n",
    "    # for col in df.columns.values:\n",
    "    #     df[col].fillna(df[col].mean(), inplace=True)\n",
    "    #     #df[col][df[col]<0] = df[col].mean()\n",
    "        \n",
    "    # abstract the data of the required length (here cosider only from 2016 to 2018)\n",
    "    df = df.loc['2016-01-01':'2018-12-31']\n",
    "   \n",
    "    \n",
    " #%%   \n",
    "    # get list of Rainfall stations associated with the flow station\n",
    "    rfst =  list(df.columns.values)\n",
    "    del rfst[0]  # the first in the list id the flow station\n",
    "    nshift = 24  ## the lag time in 5 minutes interval (2hr = 24, 1hr = 12) \n",
    "    rg_names = [] ## list of correlated rf gauging stations name\n",
    "    \n",
    "    corrST = []\n",
    "    maxcorr = []\n",
    "    indmaxcorr = [] ##index of the correlated RF station  \n",
    "    corrRG = [] \n",
    "    \n",
    "    for rg in rfst:\n",
    "        #df[rg] = np.around(df[rg].rolling(nshift).apply(np.sum), decimals=2)\n",
    "        rg_lag = rg+'_rs'+str(lag_times[f])\n",
    "        # df[rg_lag] = df[rg].rolling(nshift).sum()\n",
    "        df[rg_lag] = np.around(df[rg].rolling(lag_times[f]).apply(np.sum, raw=True), decimals=2)\n",
    "        corr = df[f].corr(df[rg_lag])\n",
    "        corrRG.append(corr)\n",
    "        rg_names.append(rg_lag)\n",
    "        sw2rg_corr = dict(zip(rg_names, corrRG))\n",
    "        \n",
    "    sorted_corr_list = sorted(sw2rg_corr, key=sw2rg_corr.get, reverse=True)\n",
    "\n",
    "    # sorted_rg = [s.replace('_lag', '') for s in sorted_corr_list]\n",
    "    # sorted_rg=sorted_corr_list\n",
    "    nrfs2c = 5  ## number of RF stations to consider    \n",
    "    # df_rg = df[sorted_rg[:nrfs2c]]\n",
    "    df_rg = df[sorted_corr_list[:nrfs2c]]\n",
    "    if(df_rg.isnull().values.any()):\n",
    "        print('there is nan in the rainfall data')\n",
    "    \n",
    "#%%\n",
    "     \n",
    "    # lag the flow series by nshift ( the duration of the forecast time)   \n",
    "    df_lag = pd.DataFrame()\n",
    "    for i in range(10):\n",
    "        #df[rg] = np.around(df[rg].rolling(nshift).apply(np.sum), decimals=2)\n",
    "        lag = 'lag_'+str(i)\n",
    "        name = f+'_lag_'+str(nshift+i)\n",
    "        df_lag[name] = df[f].shift(nshift+i)\n",
    "\n",
    "    \n",
    "    df4 = pd.concat([df[f], df_lag], axis=1)\n",
    "    if(df4.isnull().values.any()):\n",
    "        print('there is nan in df[f]')\n",
    "\n",
    "    if(not f in WL):\n",
    "        df4 = df4[df4>=0]\n",
    "\n",
    "        \n",
    "    # add the 5 most correlated rainfall stations to the flow data frame\n",
    "    df4 = pd.concat([df4, df_rg], axis=1)\n",
    "        \n",
    "    if(df4.isnull().values.any()):\n",
    "        print('there is nan in df4')\n",
    "        \n",
    "    # drop the nans\n",
    "    df4 = df4.dropna()\n",
    "    \n",
    "    print('Information of the dataframe for the modelling:\\n')\n",
    "    print(df4.info())\n",
    "    \n",
    "    # prepare the data for DDM\n",
    "    xcols =  list(df4.columns.values)\n",
    "    del xcols[0]   \n",
    "    indexData = df4.index.values\n",
    "    X = df4[xcols] #.values\n",
    "    y = df4[f] #.values\n",
    "    \n",
    "\n",
    "    ## Training data size\n",
    "    splits = TimeSeriesSplit(4) ## 3/4 for training and 1/4 for testing\n",
    "    \n",
    "    for trainIdx, testIdx in splits.split(X):\n",
    "        trainIndex = trainIdx\n",
    "        testIndex = testIdx\n",
    "       \n",
    "\n",
    "    X_train = X[:len(trainIndex)]\n",
    "    X_test = X[len(trainIndex): (len(trainIndex)+len(testIndex))]\n",
    "   \n",
    "    y_train = y[:len(trainIndex)]\n",
    "    y_test = y[len(trainIndex): (len(trainIndex)+len(testIndex))]\n",
    "   \n",
    "    print('Observations: %d' % (len(X_train)+len(X_test)))\n",
    "    print('Training Observations: %d' % len(X_train))\n",
    "    print('Testing Observations: %d' % len(X_test))\n",
    "\n",
    "      ## normilize the input features      \n",
    "    minMaxScaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    # X_train[X_train.columns] = minMaxScaler.fit_transform(X_train)\n",
    "    # X_test[X_test.columns] = minMaxScaler.transform(X_test)\n",
    "    X_train = minMaxScaler.fit_transform(X_train)\n",
    "    X_test = minMaxScaler.transform(X_test)\n",
    "    \n",
    "    file = open(name+ \"RF_parameters.txt\", \"w\")\n",
    "    \n",
    "    def evaluate(model, test_features, test_labels):\n",
    "                predictions = model.predict(test_features)\n",
    "                errors = abs(predictions - test_labels)\n",
    "                mape = 100 * np.mean(errors / test_labels)\n",
    "                accuracy = 100 - mape\n",
    "                print('Model Performance')\n",
    "                print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "                print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "                \n",
    "                return accuracy\n",
    "    \n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "    rf = RandomForestRegressor(random_state = 42)\n",
    "    \n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "    \n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "    \n",
    "    pprint(random_grid)\n",
    "    \n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestRegressor(random_state = 42)\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                                  n_iter = 100, scoring='neg_mean_absolute_error', \n",
    "                                  cv = 3, verbose=2, random_state=42, n_jobs=-2,\n",
    "                                  return_train_score=True)\n",
    "    \n",
    "    # Fit the random search model\n",
    "    rf_random.fit(X_train, y_train);\n",
    "    \n",
    "    #cell 15\n",
    "    rf_random.best_params_\n",
    "    print('Best Parameters: ',rf_random.best_params_)\n",
    "    \n",
    "    file.write(\"Random search Best Parameters:\")\n",
    "    file.write(str(rf_random.best_params_))\n",
    "\n",
    "    #cell 16\n",
    "    rf_random.cv_results_\n",
    "\n",
    "    base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "    base_model.fit(X_train, y_train)\n",
    "    base_accuracy = evaluate(base_model, X_test, y_test)\n",
    "    \n",
    "   \n",
    "    #### Evaluate the Best Random Search Model\n",
    "    best_random = rf_random.best_estimator_\n",
    "    random_accuracy = evaluate(best_random, X_test, y_test)\n",
    "    \n",
    "    print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))\n",
    "    \n",
    "#     # Grid Search \n",
    "\n",
    "#     from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "#     # Create the parameter grid based on the results of random search \n",
    "#     param_grid = {\n",
    "#         'bootstrap': [True],\n",
    "#         'max_depth': [80, 90, 100, 110],\n",
    "#         'max_features': [2, 3],\n",
    "#         'min_samples_leaf': [3, 4, 5],\n",
    "#         'min_samples_split': [8, 10, 12],\n",
    "#         'n_estimators': [100, 200, 300, 1000]\n",
    "#     }\n",
    "    \n",
    "#     # Create a base model\n",
    "#     rf = RandomForestRegressor(random_state = 42)\n",
    "    \n",
    "#     # Instantiate the grid search model\n",
    "#     grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "#                               cv = 3, n_jobs = -1, verbose = 2, return_train_score=True)\n",
    "\n",
    "#     # Fit the grid search to the data\n",
    "#     grid_search.fit(X_train, y_train);\n",
    "#     grid_search.best_params_\n",
    "    \n",
    "#     #### Evaluate the Best Model from Grid Search\n",
    "#     best_grid = grid_search.best_estimator_\n",
    "#     grid_accuracy = evaluate(best_grid, X_test, y_test)\n",
    "#     print('Best Parameters Gread Search: ',grid_search.best_params_)\n",
    "    \n",
    "#     print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))\n",
    "\n",
    "#     file.write(\"Gread search Best Parameters:\")\n",
    "#     file.write(str(grid_search.best_params_))\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c0ebf4f-ea05-49ca-84b8-bee4fe93876a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9630c30-8806-416c-9487-986b4cb6bcdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
